{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwezmTWiz7xp",
    "outputId": "68cd10b0-a50b-4aca-e456-3ecf1c5f08f8"
   },
   "outputs": [],
   "source": [
    "from ImageProcessor import *\n",
    "from Evaluation import *\n",
    "svs_path = '../prelimary_data/BRACS_1494.svs'\n",
    "json_path = '../prelimary_data/BRACS_1494.geojson'\n",
    "tile_size = 512\n",
    "\n",
    "imageProcessor_1494 = ImageProcessor(json_path, svs_path)\n",
    "tiles_1494 = imageProcessor_1494.generate_tile(tile_size = tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_path = '../prelimary_data/BRACS_1496.svs'\n",
    "json_path = '../prelimary_data/BRACS_1496.geojson'\n",
    "imageProcessor_1496 = ImageProcessor(json_path, svs_path)\n",
    "tiles_1496 = imageProcessor_1496.generate_tile(tile_size = tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_path = '../prelimary_data/BRACS_1286.svs'\n",
    "json_path = '../prelimary_data/BRACS_1286.geojson'\n",
    "imageProcessor_1286 = ImageProcessor(json_path, svs_path)\n",
    "tiles_1286 = imageProcessor_1286.generate_tile(tile_size = tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsIhGot_CJBr",
    "outputId": "b81dc4ee-9937-4f70-e6ca-598ae545e70f"
   },
   "outputs": [],
   "source": [
    "len(tiles_1494),len(tiles_1496), len(tiles_1286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = tiles_1494 + tiles_1496 + tiles_1286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOjUM2iEMS17"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base Unet Implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class:int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Unet, n_class is number of classes we want to segment images for\n",
    "        Defaults at 2 for 2 ROI types\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # In the decoder, transpose convolutional layers with the ConvTranspose2d function are used to upsample the feature maps to the original size of the input image.\n",
    "        # Each block in the decoder consists of an upsampling layer, a concatenation with the corresponding encoder feature map, and two convolutional layers.\n",
    "        # -------\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_9k_cyX6rR-k"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class SmallUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base Unet Implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class:int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Unet, n_class is number of classes we want to segment images for\n",
    "        Defaults at 2 for 2 ROI types\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "\n",
    "        self.e21 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.e22 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e31 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.e32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) #\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e41 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.e42 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e51 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.e52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # In the decoder, transpose convolutional layers with the ConvTranspose2d function are used to upsample the feature maps to the original size of the input image.\n",
    "        # Each block in the decoder consists of an upsampling layer, a concatenation with the corresponding encoder feature map, and two convolutional layers.\n",
    "        # -------\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        self.sigmoid = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        xe21 = relu(self.e21(x))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd32)\n",
    "        #out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seeds(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfDkqwFZCQF1"
   },
   "outputs": [],
   "source": [
    "tiles = [t for t in tiles if t.image_data.shape == (3, tile_size, tile_size)]\n",
    "tiles_copy = tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2uTlnYuZuaS",
    "outputId": "e8fafb62-0ea8-48b4-e46c-990b73eb60ea"
   },
   "outputs": [],
   "source": [
    "tiles_with_masks = [t for t in tiles if 1 in t.mask]\n",
    "print(len(tiles_with_masks))\n",
    "tiles = tiles_with_masks + list(np.random.choice(tiles, 3*len(tiles_with_masks), replace=False))\n",
    "print(len(tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRR6T8dnMumM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "\n",
    "def generate_random_rng_state():\n",
    "    random_seed = random.randint(0, 2**32 - 1)\n",
    "    torch.manual_seed(random_seed)\n",
    "    rng_state = torch.get_rng_state()\n",
    "    return rng_state\n",
    "\n",
    "class TiledWSIDataset(Dataset):\n",
    "    def __init__(self, tiles, length_modifier=1.0, training=True):\n",
    "        self.tiles = [t.image_data for t in tiles]\n",
    "        self.masks = [t.mask for t in tiles]\n",
    "        self.general_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            #transforms.RandomRotation(degrees=(-90, 90)),\n",
    "        ])\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            #transforms.GaussianBlur(kernel_size=(5, 5), sigma=(1.0, 2.0))\n",
    "        ])\n",
    "        self.length_modifier = length_modifier\n",
    "        self.random_state = [generate_random_rng_state()] * int(len(self.tiles) * self.length_modifier)\n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.length_modifier * len(self.tiles))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.tiles[idx % len(self.tiles)]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        \n",
    "        mask = self.masks[idx % len(self.tiles)]\n",
    "\n",
    "        # this is the most lazy possible way to do this, fix later\n",
    "        mask_rgb = np.repeat(mask, 3, axis=0)\n",
    "        mask_rgb = torch.tensor(mask_rgb)\n",
    "\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        if self.training:\n",
    "          state = self.random_state[idx]\n",
    "          torch.set_rng_state(state)\n",
    "          image = self.general_transform(image)\n",
    "\n",
    "          torch.set_rng_state(state)\n",
    "          mask_rgb = self.general_transform(mask_rgb)\n",
    "\n",
    "        mask = mask_rgb[0]\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "dataset = TiledWSIDataset(tiles, length_modifier=2.0)\n",
    "full_dataset = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "test_dataset.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCn0VpeMNI6m"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def dice_loss(pred, target):\n",
    "    smooth = 1.\n",
    "    iflat = pred.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def criterion(pred, target):\n",
    "    return dice_loss(pred, target) + F.cross_entropy(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYaN0blKNPWA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "# Split the smaller dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asZcVWL0NX6z"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(n_class=1).to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "  best_test = np.inf\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, masks in tqdm(trainloader):\n",
    "\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(images)\n",
    "\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss = loss.cpu()\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for image, masks in tqdm(testloader):\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss = loss.cpu()\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      \n",
    "    epoch_train_loss = epoch_loss / len(trainloader)\n",
    "    epoch_test_loss = test_loss / len(testloader)\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    if test_loss/len(testloader) < best_test:\n",
    "      best_test = test_loss/len(testloader)\n",
    "      lr_str = str(lr)\n",
    "      tile_size_str = str(tile_size)\n",
    "      torch.save(model.state_dict(), 'unet_' + lr_str + \"_\" + tile_size_str + 'pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss/len(trainloader)}, Test Loss: {test_loss/len(testloader)}')\n",
    "\n",
    "  return train_losses, test_losses\n",
    "\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "    # plt.plot(range(1, epochs + 1), test_losses, label='Test Loss', color='blue')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Train and Test Loss Over Epochs')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterates = 50\n",
    "UNet_trained = train(model, train_loader, test_loader, criterion, optimizer, num_iterates, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_iterates), UNet_trained[0], label='Train Loss', color='red')\n",
    "plt.plot(range(num_iterates), UNet_trained[1], label='Test Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test Loss Over Epochs with combination of 3 slides') #10^-4 \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_1 = compute_confusion_matrix(model, dataset, device)\n",
    "eval = evaluate_model(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_1 = UNet(n_class=1).to(device)\n",
    "\n",
    "lr = 1e-6\n",
    "optimizer = Adam(model_1.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "  best_test = np.inf\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, masks in tqdm(trainloader):\n",
    "\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(images)\n",
    "\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss = loss.cpu()\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for image, masks in tqdm(testloader):\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss = loss.cpu()\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      \n",
    "    epoch_train_loss = epoch_loss / len(trainloader)\n",
    "    epoch_test_loss = test_loss / len(testloader)\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    if test_loss/len(testloader) < best_test:\n",
    "      best_test = test_loss/len(testloader)\n",
    "      lr_str = str(lr)\n",
    "      tile_size_str = str(tile_size)\n",
    "      torch.save(model.state_dict(), 'unet_' + lr_str + \"_\" + tile_size_str + '_' + str(epochs) + '.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss/len(trainloader)}, Test Loss: {test_loss/len(testloader)}')\n",
    "\n",
    "  return train_losses, test_losses\n",
    "\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "    # plt.plot(range(1, epochs + 1), test_losses, label='Test Loss', color='blue')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Train and Test Loss Over Epochs')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterates = 50\n",
    "UNet_trained_1 = train(model_1, train_loader, test_loader, criterion, optimizer, num_iterates, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_iterates), UNet_trained_1[0], label='Train Loss', color='red')\n",
    "plt.plot(range(num_iterates), UNet_trained_1[1], label='Test Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test Loss Over Epochs of 50 and lr = 1e-6')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_2 = UNet(n_class=1).to(device)\n",
    "\n",
    "lr = 1e-6\n",
    "optimizer = Adam(model_2.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "  best_test = np.inf\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for images, masks in tqdm(trainloader):\n",
    "\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(images)\n",
    "\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss = loss.cpu()\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for image, masks in tqdm(testloader):\n",
    "      images, masks = images.to(device), masks.to(device)\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, masks)\n",
    "      loss = loss.cpu()\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      \n",
    "    epoch_train_loss = epoch_loss / len(trainloader)\n",
    "    epoch_test_loss = test_loss / len(testloader)\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    test_losses.append(epoch_test_loss).\n",
    "\n",
    "    if test_loss/len(testloader) < best_test:\n",
    "      best_test = test_loss/len(testloader)\n",
    "      lr_str = str(lr)\n",
    "      tile_size_str = str(tile_size)\n",
    "      torch.save(model.state_dict(), 'unet_' + lr_str + \"_\" + tile_size_str + '_' + str(epochs) + '.pth')\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss/len(trainloader)}, Test Loss: {test_loss/len(testloader)}')\n",
    "\n",
    "  total_duration = time.time() - start_time\n",
    "  total_duration_minutes = total_duration / 60\n",
    "  print(f'Total Training Time: {total_duration_minutes:.2f} minutes')\n",
    "\n",
    "  return train_losses, test_losses\n",
    "\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "    # plt.plot(range(1, epochs + 1), test_losses, label='Test Loss', color='blue')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Train and Test Loss Over Epochs')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterates = 100\n",
    "UNet_trained_2 = train(model_2, train_loader, test_loader, criterion, optimizer, num_iterates, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_iterates), UNet_trained_2[0], label='Train Loss', color='red')\n",
    "plt.plot(range(num_iterates), UNet_trained_2[1], label='Test Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test Loss Over Epochs of 100 and lr = 1e-6')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxbslWBwsH_t"
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# for images, masks in train_loader:\n",
    "#   images, masks = images.to(device), masks.to(device)\n",
    "#   outputs = model(images)\n",
    "\n",
    "#   outputs = outputs.cpu()\n",
    "#   masks = masks.cpu()\n",
    "\n",
    "#   for i in range(outputs.shape[0]):\n",
    "\n",
    "#     threshold = 0.5\n",
    "#     test_output = outputs[i].detach().numpy().transpose(1, 2, 0)\n",
    "#     binary_output = np.zeros(test_output.shape, dtype=np.uint8)\n",
    "#     binary_output[test_output > threshold] = 1\n",
    "\n",
    "#     fig, ax = plt.subplots(1, 4, figsize=(8, 3))\n",
    "\n",
    "#     image = images.cpu()\n",
    "\n",
    "#     image = image[i].detach().numpy().transpose(1, 2, 0)\n",
    "#     print(np.max(image))\n",
    "#     print(np.min(image))\n",
    "#     ax[0].imshow(image, cmap='viridis')\n",
    "#     ax[0].set_title('Input')\n",
    "\n",
    "#     ax[1].imshow(test_output, cmap='viridis')\n",
    "#     ax[1].set_title('True Output')\n",
    "\n",
    "#     ax[2].imshow(binary_output, cmap='viridis')\n",
    "#     ax[2].set_title('Binary Output')\n",
    "\n",
    "#     # just to stop strange colour things going on\n",
    "#     current_mask = masks[i].detach().numpy().transpose(1, 2, 0)\n",
    "#     if 0 not in current_mask:\n",
    "#       current_mask[0][0] = 0\n",
    "\n",
    "#     if 1 not in current_mask:\n",
    "#       current_mask[0][0] = 1\n",
    "\n",
    "#     ax[3].imshow(current_mask, cmap='viridis')\n",
    "#     ax[3].set_title('Mask')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#   #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9N4y5DoNrzmd"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unet.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
